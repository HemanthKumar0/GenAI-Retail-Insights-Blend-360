# Retail Insights Assistant â€” Complete Project Walkthrough

## The Big Picture

User opens a Streamlit web app â†’ uploads a CSV â†’ types a question like "What are the top 5 products by revenue?" â†’ the system converts that English into SQL using an LLM â†’ runs it on DuckDB â†’ validates the results â†’ uses the LLM again to write a nice English answer â†’ shows it in the chat.

---

## File-by-File Breakdown

### `app.py` â€” The Streamlit UI (entry point)

This is what the user sees. You run `streamlit run app.py`.

- `initialize_session_state()` â€” creates the orchestrator once and stores it in Streamlit's session. Also initializes chat history, mode, loaded datasets. **No LLM used.**
- `display_header()` â€” renders the title and subtitle. Pure UI.
- `display_sidebar()` â€” shows mode toggle (Q&A vs Summarization), file uploader, loaded datasets list, API key status. When a file is uploaded, it calls `data_store.load_csv()` / `load_excel()` / `load_json()` to load it into DuckDB. **No LLM used.**
- `display_chat_history()` â€” loops through `st.session_state.messages` and renders each one as a chat bubble with metadata (time, tokens, cached). Pure UI.
- `process_user_input(user_input)` â€” the core action. Takes the user's typed question, calls `orchestrator.process_query(user_input, mode)`, displays the response, and appends both user + assistant messages to history. **This is where the LLM pipeline kicks off.**
- `display_example_queries()` â€” shows clickable example buttons. Pure UI.
- `main()` â€” ties everything together. Calls all the above in order.

---

### `src/utils/app_utils.py` â€” App Initialization

Single function:
- `create_orchestrator()` â€” reads `Config` to decide OpenAI vs Gemini, creates the LLM provider, creates `DataStore`, creates all 3 agents, wires them into the `Orchestrator`. **No LLM calls here**, just object construction.

---

### `src/core/config.py` â€” Configuration

- `Config` class â€” reads `.env` file via `python-dotenv`. Holds all settings: API keys, model names, timeouts, cache size, RAG toggle, etc. **No LLM used.**
- `setup_logging()` â€” configures Python logging to console + `retail_insights.log` file. Called on import.

---

### `src/core/models.py` â€” Data Models (the "language" of the system)

All plain Python dataclasses. **No LLM used anywhere.** These are just data containers passed between agents:

- `Message` â€” a chat message (role, content, timestamp, metadata)
- `StructuredQuery` â€” what the QueryAgent produces: `operation_type` (sql/pandas/semantic), `operation` (the actual SQL string), `explanation`, `parameters`
- `QueryResult` â€” what the ExtractionAgent produces: a DataFrame + row_count + execution_time + cached flag
- `Anomaly` â€” a data quality issue (type, description, severity, affected_rows)
- `ValidationResult` â€” what the ValidationAgent produces: passed/failed + issues list + anomalies + confidence score
- `Response` â€” the final answer: natural language text + optional DataFrame + metadata
- `ColumnInfo`, `TableSchema`, `DataSchema` â€” schema descriptions of loaded tables

---

### `src/core/orchestrator.py` â€” The Brain (LangGraph)

This is the most important file. It coordinates everything using a LangGraph `StateGraph`.

**The graph looks like this:**
```
parse_query â†’ execute_query â†’ validate_results â†’ [conditional]
                                                    â”œâ”€ PASSED â†’ format_response â†’ END
                                                    â”œâ”€ FAILED (retries left) â†’ reformulate_query â†’ parse_query (loop back)
                                                    â””â”€ FAILED (max retries) â†’ error_response â†’ END
```

**`AgentState`** â€” a TypedDict that flows through every node. Contains: user_query, current_query, schema, structured_query, query_result, validation_result, attempt count, response, error.

**Key functions:**

- `__init__()` â€” stores all agents, builds the LangGraph once via `_build_graph()`. **No LLM call.**
- `_build_graph()` â€” registers 6 nodes and their edges, compiles the graph. **No LLM call.**
- `process_query(user_query, mode)` â€” the public entry point. Adds user message to context, builds initial state dict, calls `self._graph.invoke(initial_state)`, adds assistant response to context. **This triggers the entire LLM pipeline.**
- `_node_parse_query(state)` â€” calls `query_agent.parse_query()`. **ğŸ”´ USES LLM** (inside QueryAgent)
- `_node_execute_query(state)` â€” calls `extraction_agent.execute_query()`. **No LLM.** Pure DuckDB/Pandas.
- `_node_validate_results(state)` â€” calls `validation_agent.validate_results()`. **No LLM.** Pure math/logic checks.
- `_route_after_validation(state)` â€” decides: format_response vs reformulate vs error. **No LLM.** Just if/else logic.
- `_node_format_response(state)` â€” calls `_format_response()` which uses LLM to write the English answer. **ğŸ”´ USES LLM**
- `_node_reformulate_query(state)` â€” calls `_reformulate_query()` which asks LLM to rewrite the query. **ğŸ”´ USES LLM**
- `_node_error_response(state)` â€” builds a static error message. **No LLM.**
- `_get_data_schema()` â€” reads table schemas from DataStore. **No LLM.**
- `_log_communication()` â€” appends to communication_log list. **No LLM.**
- `_create_data_summary()` â€” converts DataFrame to text for the LLM prompt. **No LLM** (just string formatting).
- `reset_context()`, `get_conversation_history()`, `get_communication_log()`, `clear_communication_log()` â€” simple getters/setters. **No LLM.**

---

### `src/agents/query_agent.py` â€” Agent 1: Natural Language â†’ SQL

This is where English becomes SQL. **ğŸ”´ Heavy LLM usage.**

- `__init__(llm_provider)` â€” stores the LLM provider.
- `parse_query(query, schema, context)` â€” the main function. Formats a prompt with the schema + conversation context + user query, sends it to the LLM, gets back JSON like `{"operation_type": "sql", "operation": "SELECT ...", "explanation": "..."}`, parses it into a `StructuredQuery`. **ğŸ”´ USES LLM**
- `_parse_llm_response(response_text)` â€” extracts JSON from LLM output (handles markdown code blocks), validates required fields, creates `StructuredQuery`. **No LLM** (just parsing).
- `_extract_json(text)` â€” regex to pull JSON from markdown blocks. **No LLM.**
- `_schema_to_dict(schema)` â€” converts DataSchema to dict for the prompt. **No LLM.**

---

### `src/agents/extraction_agent.py` â€” Agent 2: Execute the Query

This agent runs the actual SQL/Pandas against your data. **No LLM used at all.**

- `__init__(data_store)` â€” stores DataStore reference, sets up LRU cache.
- `execute_query(query: StructuredQuery)` â€” the main function. Checks cache first, then routes to `_execute_sql_query()` or `_execute_pandas_query()` based on `operation_type`. Applies timeout check and pagination (caps at 10,000 rows). Caches the result.
- `_execute_sql_query(query)` â€” calls `data_store.execute_sql(query.operation)`. DuckDB runs the SQL.
- `_execute_pandas_query(query)` â€” uses `eval()` with restricted namespace to run Pandas expressions on the DataFrame.
- `_get_cache_key(query)` â€” MD5 hash of the query for caching.
- `_get_from_cache()` / `_add_to_cache()` â€” LRU cache management.
- `clear_cache()` â€” empties the cache.

---

### `src/agents/validation_agent.py` â€” Agent 3: Check the Results

Validates data quality. **No LLM used at all.** Pure math and logic.

- `__init__(business_rules)` â€” loads default or custom business rules.
- `validate_results(results, query)` â€” runs 5 checks in sequence, collects issues and anomalies, calculates confidence score, returns `ValidationResult(passed=True/False)`.
- `_validate_data_types(df)` â€” checks for object columns that should be numeric, datetime columns with NaT values.
- `_check_mathematical_consistency(df)` â€” checks: negative values in sales columns, total â‰  subtotal + tax, amount â‰  price Ã— quantity.
- `_validate_empty_results(results, query)` â€” if 0 rows returned, checks if WHERE/JOIN/HAVING might be too restrictive.
- `_detect_anomalies(df)` â€” finds negative sales, invalid dates (NaT, out-of-range 1900-2100).
- `_calculate_confidence(issues, anomalies)` â€” starts at 1.0, subtracts 0.1 per issue, 0.2 per error anomaly, 0.05 per warning.
- `check_business_rules(results)` â€” validates category values against known list, checks sales/order count consistency, checks sales within min/max bounds.

---

### `src/data/data_store.py` â€” Data Loading & SQL Engine

Manages DuckDB in-memory database. **No LLM used.**

- `__init__()` â€” creates DuckDB in-memory connection.
- `register_dataframe(table_name, df)` â€” registers a Pandas DataFrame as a DuckDB table.
- `execute_sql(query)` â€” runs SQL on DuckDB, auto-converts backticks to double quotes, returns DataFrame.
- `get_table_schema(table_name)` â€” returns column names, types, nullable flags, row count.
- `list_tables()` â€” returns list of registered table names.
- `load_csv(file_path)` â€” loads CSV. If >1GB, uses `_load_csv_chunked()` which reads in 100K-row chunks and appends to DuckDB table.
- `load_excel(file_path)` â€” loads each sheet as a separate table.
- `load_json(file_path)` â€” loads JSON with `_normalize_json()` to flatten nested structures.
- `close()` â€” closes DuckDB connection.

---

### `src/data/context_manager.py` â€” Conversation Memory

Manages chat history within token limits. **ğŸ”´ Uses LLM only for summarization.**

- `ContextWindow` â€” low-level storage. `add_message()`, `get_context_string()` (truncates to fit token budget, prioritizes recent messages), `estimate_tokens()`.
- `ContextManager` â€” high-level wrapper. `add_message()` checks if context is >80% full and triggers `_summarize_old_context()`. `get_context()` returns the context string.
- `_summarize_old_context()` â€” keeps last 3 messages, sends older ones to LLM with a summarization prompt, stores the summary. **ğŸ”´ USES LLM**

---

### `src/llm/llm_provider.py` â€” LLM Abstraction Layer

- `LLMResponse` â€” dataclass: content, tokens_used, model, cached flag.
- `LLMProvider` (abstract base) â€” defines `generate()`, `count_tokens()`, `generate_with_cache()`, `generate_with_retry()`.
- `generate_with_cache()` â€” SHA256 hashes the prompt+params, checks in-memory dict cache, returns cached response or generates new one.
- `generate_with_retry()` â€” exponential backoff (1s, 2s, 4s) on failures.
- `GeminiProvider` â€” implements `generate()` using `google.generativeai`, `count_tokens()` using Gemini's tokenizer.
- `OpenAIProvider` â€” implements `generate()` using `openai.ChatCompletion`, `count_tokens()` using `tiktoken`.
- `LLMProviderFactory` â€” `create_provider("gemini"/"openai", model, api_key)`.

---

### `src/llm/prompt_templates.py` â€” All LLM Prompts

Contains every prompt template used in the system. **No LLM calls here** â€” just string templates.

- `QUERY_PARSING_PROMPT` â€” the big one. Includes schema, context, user query, 7 few-shot examples showing how to convert English â†’ SQL/Pandas/semantic. Includes DuckDB-specific rules (double quotes, TRY_CAST).
- `SUMMARIZATION_PROMPT` â€” tells LLM to write a business summary with sections (Overview, Key Metrics, Top Performers, Concerns, Recommendations).
- `CONTEXT_SUMMARIZATION_PROMPT` â€” summarizes old conversation history.
- `format_*()` static methods â€” fill in the templates with actual data.

---

### `src/modes/qa_mode.py` â€” Q&A Mode

- `answer_question(question)` â€” checks if it's a clarification request, otherwise calls `orchestrator.process_query(question, mode="qa")`. **ğŸ”´ USES LLM** (via orchestrator)
- `_is_clarification_request(question)` â€” keyword matching for "explain", "clarify", "tell me more", etc. **No LLM.**
- `_handle_clarification(question)` â€” gets last assistant response from history, asks LLM to elaborate. **ğŸ”´ USES LLM**

---

### `src/modes/summarization_mode.py` â€” Summarization Mode

- `generate_summary(table_name)` â€” the main function. Calls 4 analysis methods, then formats with LLM.
- `_get_dataset_info()` â€” basic stats (row count, columns, date range). **No LLM.**
- `_calculate_key_metrics()` â€” sums, averages, min/max for numeric columns. Calculates YoY growth. **No LLM.**
- `_calculate_yoy_growth()` â€” groups by year, computes percentage change. **No LLM.**
- `_identify_performers()` â€” groups by first categorical column, sorts by first numeric column, gets top 5 and bottom 5. **No LLM.**
- `_detect_trends()` â€” IQR outlier detection, rolling average trend direction (increasing/decreasing/stable). **No LLM.**
- `_format_summary()` â€” sends all the above data to LLM with the summarization prompt template. **ğŸ”´ USES LLM**
- `_generate_fallback_summary()` â€” if LLM fails, builds a basic text summary. **No LLM.**

---

## The Complete Flow (step by step)

Here's what happens when you type "What are the top 5 products by revenue?":

```
1. app.py: process_user_input() receives the text
2. app.py: calls orchestrator.process_query("What are the top 5...", mode="qa")
3. orchestrator.py: adds user message to ContextManager
4. orchestrator.py: reads table schema from DataStore (column names, types)
5. orchestrator.py: gets conversation context string from ContextManager
6. orchestrator.py: builds initial AgentState dict, calls self._graph.invoke()

   --- LangGraph starts ---

7. NODE parse_query:
   â†’ QueryAgent.parse_query() is called
   â†’ Builds a prompt with schema + context + user question + few-shot examples
   â†’ ğŸ”´ Sends prompt to LLM (OpenAI/Gemini)
   â†’ LLM returns JSON: {"operation_type": "sql", "operation": "SELECT product,
     SUM(sales) as revenue FROM sales GROUP BY product ORDER BY revenue DESC
     LIMIT 5", "explanation": "Top 5 products by revenue"}
   â†’ Parses JSON into StructuredQuery object

8. NODE execute_query:
   â†’ ExtractionAgent.execute_query() is called
   â†’ Checks LRU cache (miss on first run)
   â†’ Runs SQL on DuckDB: SELECT product, SUM(sales)...
   â†’ Gets back a Pandas DataFrame with 5 rows
   â†’ Wraps in QueryResult (data, row_count=5, execution_time=0.02s)
   â†’ Stores in cache

9. NODE validate_results:
   â†’ ValidationAgent.validate_results() is called
   â†’ Check 1: data types OK
   â†’ Check 2: no negative values in sales column
   â†’ Check 3: not empty (5 rows)
   â†’ Check 4: no anomalies
   â†’ Check 5: business rules OK
   â†’ Returns ValidationResult(passed=True, confidence=1.0)

10. ROUTING _route_after_validation:
    â†’ validation.passed is True â†’ go to "format_response"

11. NODE format_response:
    â†’ Builds a text summary of the 5-row DataFrame
    â†’ ğŸ”´ Sends to LLM with response formatting prompt
    â†’ LLM writes: "The top 5 products by revenue are:
      1. Product X ($50,000)..."
    â†’ Wraps in Response object with metadata

    --- LangGraph ends ---

12. orchestrator.py: adds assistant message to ContextManager
13. app.py: displays the answer in chat bubble
14. app.py: shows the DataFrame as a table below the answer
15. app.py: shows metadata (execution time, tokens used)
```

**If validation FAILS** (say the SQL returned negative sales values):
- Step 10 routes to `reformulate_query` instead
- **ğŸ”´ LLM** rewrites the query to fix the issue
- Loops back to step 7 with the new query
- Tries up to 3 times total
- If all 3 fail â†’ `error_response` node builds a static error message

---

## Summary: Where LLM is Used vs Not

| Step | LLM? | What does it |
|---|---|---|
| File upload & loading | âŒ | Pandas/DuckDB reads CSV/Excel/JSON |
| Schema detection | âŒ | Pandas dtype inspection |
| English â†’ SQL conversion | âœ… | QueryAgent sends prompt to LLM |
| SQL execution | âŒ | DuckDB runs the SQL |
| Result validation | âŒ | Math checks, type checks, business rules |
| Query reformulation (on retry) | âœ… | LLM rewrites the failed query |
| Response formatting | âœ… | LLM writes the English answer |
| Summarization analysis | âŒ | Pandas groupby, aggregation, IQR |
| Summary formatting | âœ… | LLM writes the summary text |
| Context summarization | âœ… | LLM summarizes old chat history |
| Error messages | âŒ | Hardcoded templates |
| Caching | âŒ | SHA256 hash + in-memory dict |
